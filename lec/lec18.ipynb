{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Lecture 18 \n",
    "\n",
    "Data Science 8, Spring 2021 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<b>\n",
    "<ul>\n",
    "<li>Hypothesis Testing and $p$-values   </li><br>\n",
    "            \n",
    "<li>Making Decisions with Incomplete Information  </li><br>\n",
    "\n",
    "<li>Error Probabilities  </li><br>\n",
    "</ul>\n",
    "</b>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "plots.rcParams[\"patch.force_edgecolor\"] = True\n",
    "\n",
    "#The following allows porting images into a Markdown window\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GSI's Defense ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = Table.read_table('scores_by_section.csv')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Headcounts of each of the twelve sections</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that if we don't pass to the group method anything other than the \n",
    "# column label, it will simply return headcounts for each category.\n",
    "# Here the categories are the section numbers. \n",
    "section_headcounts = scores.group('Section')\n",
    "section_headcounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compute the average score for each section</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass on the function np.average as a second argument, and \n",
    "# the group method will return the average score for each section.\n",
    "section_averages = scores.group('Section', np.average)\n",
    "section_averages.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3's Midterm Average</h4>\n",
    "\n",
    "This is our observed test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_average = section_averages.column('Midterm average').item(2) \n",
    "observed_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3's Population (Head Count)</h4>\n",
    "\n",
    "This is our sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = section_headcounts.column('count').item(2)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In our random selection, we do NOT want to select the same student \n",
    "# more than once.  So, we must sample withOUT replacement.\n",
    "random_sample = scores.sample(sample_size, with_replacement=False)\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What we've done in the cell above constitutes running ONE trial. <br>\n",
    "    \n",
    "We must run a large number of trials so we can construct an empirical distribution. </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Random Sample's Average Score: <br>\n",
    "    \n",
    "This is our Simulated Test Statistic</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_average_score = np.average(random_sample.column('Midterm'))\n",
    "random_sample_average_score\n",
    "# If you wish to round, uncomment the line below\n",
    "#np.round(random_sample_average_score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compare with the observed statistic&mdash;average of scores in Sec. 3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_average\n",
    "# If you wish to round, uncomment the line below\n",
    "#np.round(observed_average,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Doesn't look very far, but we still don't know what \"far\" means here.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define a function that does the above:</h3><br>\n",
    "\n",
    "<h4>Creates a random sample of size <tt>sample_size</tt> from the roster,  <br>\n",
    "\n",
    "and computes a section average score.<br>\n",
    "\n",
    "Each run of this function constitutes one \"trial.\"</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate one value of the test statistic \n",
    "# under the hypothesis that the section is like a random sample from the class\n",
    "\n",
    "def random_sample_midterm_avg(sample_size):\n",
    "    random_sample = scores.sample(sample_size, with_replacement = False)\n",
    "    return np.average(random_sample.column('Midterm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's run the cell a few times to check if it's generating varying numbers.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_midterm_avg(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The next cell runs the \"trial\" many, many times&mdash;in fact, <tt>num_simulations</tt> times.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 50,000 copies of the test statistic\n",
    "\n",
    "num_simulations= 50000\n",
    "\n",
    "# Create an empty array that will ultimately contain \n",
    "# the sample average for each of the trials.\n",
    "sample_averages = make_array()\n",
    "\n",
    "for i in np.arange(num_simulations):\n",
    "    new_sample_average=random_sample_midterm_avg(sample_size)\n",
    "    sample_averages = np.append(sample_averages, new_sample_average)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Our Decision: </h3>\n",
    "\n",
    "<h3>Compare the simulated distribution of the statistic and the actual, observed statistic </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create a table containing the sample averages.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_table = Table().with_column('Random Sample Average', sample_averages)\n",
    "#averages_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plot a histogram of the sample averages<br>\n",
    "    \n",
    "Superimpose on the histogram the value of the actual, observed statistic (red dot)\n",
    "    \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "averages_table.hist(bins = 20)\n",
    "# Plot a red dot of size 120, at vertical coordinate -0.01, \n",
    "# which is just under the horizontal axis\n",
    "plots.scatter(observed_average, -0.01, color='red', s=120);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Question: Does Sec. 3's average (red dot) <br>\n",
    "seem very different from the rest of the sections?</h3>\n",
    "\n",
    "<h4>This is an example where each answer&mdash;'yes' or 'no'&mdash;seems plausible. <br>\n",
    "    \n",
    "This is why we have to define terms such as \"different,\" \"close,\" or \"far\" more concretely.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLIDE: Statistical Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conventions About Inconsistency</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Approach I: Determine fraction of sample values $\\leq$ observed value.<br><br>\n",
    "    \n",
    "If the fraction is less than 5%, the observed value is a statistical outlier.<br><br>\n",
    "    \n",
    "    \n",
    "Note: The observed average is the average score of Sec. 3.</h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> How many of our sample averages (simulated) <br><br>\n",
    "are less than, or equal to, the observed average? </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_averages <= observed_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>To determine the count of sample averages $\\leq$ observed average, use <tt>sum</tt>.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_head_count = sum(sample_averages <= observed_average)\n",
    "tail_head_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The tail probability is the ratio of the tail head count<br><br> \n",
    "over <tt>num_simulations</tt>, the total number of samples (trials).<br><br> \n",
    "\n",
    "The tail probability is also called the $p$-value.<br><br> \n",
    "    It's the probability of obtaining results <u>at least as extreme</u> as the observed value.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Calculate the p-value: simulation area beyond observed value\n",
    "tail_probability=tail_head_count/num_simulations\n",
    "tail_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>This is NOT less than 5%.  So, the GSI's assertion is supported by the data.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Alternative Python code for calculating the tail probability:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Calculate the p-value: simulation area beyond observed value\n",
    "np.count_nonzero(sample_averages <= observed_average) / num_simulations\n",
    "# (2) See if this is less than 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Where is the 5% cutoff?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the Averages Table\n",
    "averages_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sort the table from low to high values:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_averages_table = averages_table.sort(0)\n",
    "sorted_averages_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Grab the boundary value <u>beyond</u> which the tail probability $<$ 0.05 (5%):</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Find simulated value corresponding to 5% of 50,000 = 2500\n",
    "five_percent_point = sorted_averages_table.column(0).item(2500)\n",
    "five_percent_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>If Sec. 3's average score (i.e., observed value) <br><br>\n",
    "had been lower than the 5%-point above, <br><br>\n",
    "it would have contradicted the GSI's claim.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) See if this value is greater than observed value\n",
    "observed_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>It was close, but the GSI Wins by scraping by!<br><br>\n",
    "I still would NOT want to be in that GSI's section!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_table.hist(bins = 20)\n",
    "# Plot a gold-colored vertical line, of thickness 2, \n",
    "# at horizontal coordinate equal to the \n",
    "# five_percent_point (which is 13.63), \n",
    "# from vertical coordinate 0 to 35 (%/unit) (0 to 0.35)\n",
    "plots.plot([five_percent_point, five_percent_point], [0, 0.35], color='gold', lw=2)\n",
    "\n",
    "# Give the plot a title\n",
    "plots.title('Area to the left of the gold line is 5%');\n",
    "\n",
    "# Plot a red dot of size 120, at vertical coordinate -0.01, \n",
    "# which is just under the horizontal axis\n",
    "plots.scatter(observed_average, -0.01, color='red', s=120);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>QUESTION: Did Sec. 3's average score fall in the 5% tail?</h3>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
